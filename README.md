# Bert-fused-amharic

The BERT-fused Amharic-English model or architecture refers to a machine translation system that integrates BERT (Bidirectional Encoder Representations from Transformers) into a neural machine translation (NMT) framework, specifically for translating between Amharic and English. This architecture aims to leverage the deep contextual embeddings generated by BERT to enhance the translation quality between these languages. Here's a detailed explanation of its components and how it functions:

### BERT Background
BERT is a pre-trained deep learning model developed by Google that has revolutionized the field of natural language processing (NLP). It uses a transformer-based architecture to generate contextualized word embeddings, meaning that the representation of each word takes into account the entire context of a sentence. This allows for a more nuanced understanding of language, capturing aspects like polysemy and syntax.

### Neural Machine Translation (NMT)
NMT is a method for automatic translation that uses deep neural networks, particularly those based on the transformer architecture. Unlike traditional statistical machine translation, NMT models the entire translation process as a single, end-to-end system, learning to map sequences of words from a source language to a target language.

### BERT-fused Architecture for Amharic-English Translation
The BERT-fused Amharic-English model incorporates BERT embeddings into the NMT framework, utilizing the strengths of BERT's contextualized embeddings to improve translation between Amharic and English. The architecture typically follows these steps:

1. Pre-training: BERT is pre-trained on large corpora of text in both Amharic and English to learn deep, contextualized representations of the languages. This involves training on tasks like masked language modeling and next sentence prediction.
2. Integration into NMT: The pre-trained BERT model is then integrated into the encoder and decoder parts of a transformer-based NMT system. There are different strategies for integration, including using BERT embeddings as additional input features or initializing the NMT model's embedding layers with weights from BERT.
3. Fusion Mechanism: The key aspect of the BERT-fused model is how the BERT embeddings are combined with the NMT model's internal representations. This can be done through techniques like attention mechanisms, where the NMT model learns to weigh the importance of BERT's contextual embeddings during the training process.
4. Fine-tuning: The entire system is then fine-tuned on parallel corpora of Amharic and English sentences. This step adjusts the pre-trained BERT weights and the NMT model to better perform the translation task.

### Framework Architecture
The Amharic BERT-fused model is built using the fairseq toolkit, adopting the BERT-fused approach from the [bert-nmt](https://github.com/bert-nmt/bert-nmt) repository as detailed in the [INCORPORATING BERT INTO NEURAL MACHINE TRANSLATION](https://arxiv.org/abs/2002.06823) paper.

### Requirements and Installation:
PyTorch version >= 1.13.0 \
Python version >= 3.8

Cloning and Building the Repository:

Repo cloning:
```commandline
git clone https://github.com/wubet/bert-fused-amharic-with-vanilla.git
```
Installs all the dependencies
```commandline
pip3 install -r requirements.txt
```

Installs the build module, a modern Python package builder
```commandline
pip3 install build
```

Builds the package, generating distribution archives (wheel and source) in the dist directory
```commandline
python3 -m build
```

Installs the package in editable mode (also known as development mode), allowing changes to the code to be immediately reflected
```commandline
pip3 install --editable .
```

Compiles and builds any extension modules (e.g., C extensions) specified in setup.py directly in the source directory
```commandline
python3 setup.py build_ext –inplace
```

### Data Preparation

This step includes cleaning the data (removing unnecessary characters, normalizing text, etc.), tokenizing sentences (breaking text down into smaller parts like words or subwords), and applying more advanced text processing techniques to improve model training efficiency. 

Clone the English-Amharic corpus.
```commandline
Git clone https://github.com/wubet/unified-amharic-english-corpus.git
```

We need to transliterate the Gee'z character representation into latin character representation.

For development or validation
```buildoutcfg
python3 translitration/create_transliteration.py \
  --original_filenames=unified-amharic-english-corpus/datasets/dev.am-en.base.am \
  --transliterate_filenames=unified-amharic-english-corpus/datasets/dev.am-en.transliteration.am \
```

For training
```buildoutcfg
python3 translitration/create_transliteration.py \
  --original_filenames=unified-amharic-english-corpus/datasets/train.am-en.base.am \
  --transliterate_filenames=unified-amharic-english-corpus/datasets/train.am-en.transliteration.am \
```

For testing
```buildoutcfg
python3 translitration/create_transliteration.py \
  --original_filenames=tf-transformer/unified-amharic-english-corpus/datasets/test.am-en.base.am \
  --transliterate_filenames=unified-amharic-english-corpus/datasets/test.am-en.transliteration.am \
```

Process the corpus data for training and evaluation. Typically, processing encompasses various crucial stages to adeptly navigate the intricacies of language. Such stages encompass breaking down words into subwords or tokens, mapping these tokens to a specific vocabulary, and incorporating special tokens, all aimed at enhancing the model's proficiency in dealing with infrequent words and morphological differences.

Preprocessing Training data for NMT:
```commandline
python3 data/bilingual_data_processor.py \
--en_file="unified-amharic-english-corpus/datasets/train.am-en.base.en"
--am_file="unified-amharic-english-corpus/datasets/train.am-en.transliteration.am"
--implementation=""
--data_bin_path="data-bin/bert/wmt23_en_am"
--task_file="train.en-am"
```

Preprocessing Training data for Bert:
```commandline
python3 data/bilingual_data_processor.py \
--en_file="unified-amharic-english-corpus/datasets/train.am-en.base.en"
--am_file="unified-amharic-english-corpus/datasets/train.am-en.transliteration.am"
--data_bin_path="data-bin/bert/wmt23_en_am"
--task_file="train.bert.en-am"
--use_bert_tokenizer
```

Preprocessing Validation Data for NMT:
```commandline
python3 data/bilingual_data_processor.py \
--en_file="unified-amharic-english-corpus/datasets/dev.am-en.base.en"
--am_file="unified-amharic-english-corpus/datasets/dev.am-en.transliteration.am"
--data_bin_path="data-bin/bert/wmt23_en_am"
--task_file="valid.en-am"
```

Preprocessing Validation Data for Bert:
```commandline
python3 data/bilingual_data_processor.py \
--en_file="unified-amharic-english-corpus/datasets/dev.am-en.base.en"
--am_file="unified-amharic-english-corpus/datasets/dev.am-en.transliteration.am"
--data_bin_path="data-bin/bert/wmt23_en_am"
--task_file="valid.bert.en-am"
--use_bert_tokenizer
```

Preprocessing Test Data for NMT:
```commandline
python3 Data/bilingual_data_processor.py \
--en_file="unified-amharic-english-corpus/datasets/test.am-en.base.en"
--am_file="unified-amharic-english-corpus/datasets/test.am-en.transliteration.am"
--data_bin_path="data-bin/bert/wmt23_en_am"
--task_file="test.en-am"
```

Preprocessing Test Data for Bert:
```commandline
python3 Data/bilingual_data_processor.py \
--en_file="unified-amharic-english-corpus/datasets/test.am-en.base.en"
--am_file="unified-amharic-english-corpus/datasets/test.am-en.transliteration.am"
--data_bin_path="data-bin/bert/wmt23_en_am"
--task_file="test.bert.en-am"
--use_bert_tokenizer
```

Preparing Amharic translitration file
```commandline
python3 translitration/create_transliteration.py
--source_filenames=unified-amharic-english-corpus/datasets/train.am-en.base.en \
--target_filenames=unified-amharic-english-corpus/datasets/train.am-en.transliteration.am
```

### Train the Model
Training a BERT-fused Amharic-English model involves a few critical steps, and depending on your starting point, you can either begin with pre-existing checkpoint data to "warm up" the training or start from scratch. Here’s a detailed explanation of both approaches:
1. Starting with Checkpoint Data (Warm-up Approach)

Using checkpoint data, such as checkpoint_best.pt obtained from previous Vanilla Transformer training, can significantly accelerate the training process by providing a solid foundation of learned weights. This method leverages the knowledge already acquired by the Transformer model to improve the BERT-fused model's performance from the get-go.
If you want to go this approach you can start training from [vanilla transformer model](./fairseq_cli/README.md)
2. Starting from Scratch (Zero Initialization)

Starting from scratch means initializing the BERT-fused model's weights randomly, without leveraging any pre-learned knowledge from previous training sessions. This approach requires more time and computational resources but can be beneficial for exploring new architectures or when prior training data is not available or relevant.

In order to train the model use the following command:
```commandline
python3 run_trainer.py \
--arch=transformer_wmt_en_am \
--optimizer=adam \
--lr=0.0005 \
-s=en \
-t=am \
--label-smoothing=0.1 \
--dropout=0.3 \
--max-tokens=4000 \
--min-lr=1e-09 \
--lr-scheduler=inverse_sqrt \
--weight-decay=0.0001 \
--criterion=label_smoothed_cross_entropy \
--max-update=1000000 \
--warmup-updates=4000 \
--warmup-init-lr=1e-07 \
"--adam-betas=(0.9,0.98)" \
--save-dir=checkpoints/iwed_en_am_0.5 \
--encoder-bert-dropout \
--encoder-bert-dropout-ratio=0.5 \
--save-interval=50 \
--keep-interval-updates=5 \
data-bin/bert/wmt23_en_am
```